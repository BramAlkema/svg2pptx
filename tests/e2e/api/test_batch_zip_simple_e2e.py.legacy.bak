#!/usr/bin/env python3
"""
Simplified E2E tests for ZIP Structure Preservation in Google Drive.

Tests core ZIP file upload and structure preservation functionality:
- Basic ZIP structure preservation
- File organization in Drive folders
- ZIP upload workflow coordination
"""

import pytest
from pathlib import Path
from unittest.mock import patch

# Import test infrastructure
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from api.main import app
from api.auth import get_current_user
from src.batch.models import BatchJob, BatchDriveMetadata, BatchFileDriveMetadata
from tests.e2e.api.test_batch_drive_e2e import BatchDriveE2EFixtures


class TestZIPStructureBasicE2E(BatchDriveE2EFixtures):
    """Test basic ZIP structure preservation in Google Drive."""
    
    def setup_method(self):
        """Set up test environment."""
        async def override_auth():
            return {'api_key': 'e2e_zip_key', 'user_id': 'e2e_zip_user'}
        
        app.dependency_overrides[get_current_user] = override_auth
    
    def teardown_method(self):
        """Clean up after tests."""
        app.dependency_overrides.clear()
    
    def test_zip_basic_structure_preservation_e2e(self, client, test_db_path):
        """Test basic ZIP structure preservation workflow."""
        with patch('api.routes.batch.DEFAULT_DB_PATH', test_db_path):
            job_id = "zip_basic_test"
            
            # Create batch job for ZIP processing
            batch_job = BatchJob(
                job_id=job_id,
                status="completed",
                total_files=5,
                drive_integration_enabled=True
            )
            batch_job.save(test_db_path)
            
            # Create Drive metadata 
            drive_metadata = BatchDriveMetadata(
                batch_job_id=job_id,
                drive_folder_id="zip_basic_folder_123",
                drive_folder_url="https://drive.google.com/drive/folders/zip_basic_folder_123"
            )
            drive_metadata.save(test_db_path)
            
            # Add file metadata for files from ZIP structure
            test_files = [
                ("root_file.svg", "file_root_123"),
                ("icons/home.svg", "file_icons_home"), 
                ("icons/user.svg", "file_icons_user"),
                ("graphics/logo.svg", "file_graphics_logo"),
                ("diagrams/flow.svg", "file_diagrams_flow")
            ]
            
            for original_filename, file_id in test_files:
                file_metadata = BatchFileDriveMetadata(
                    batch_job_id=job_id,
                    original_filename=original_filename,
                    drive_file_id=file_id,
                    drive_file_url=f"https://drive.google.com/file/d/{file_id}/view",
                    upload_status="completed"
                )
                file_metadata.save(test_db_path)
            
            # Test ZIP structure workflow
            response = client.get(f"/batch/jobs/{job_id}/drive-info")
            assert response.status_code == 200
            
            data = response.json()
            assert data["drive_folder_id"] == "zip_basic_folder_123"
            assert len(data["uploaded_files"]) == 5
            
            # Verify all files uploaded successfully (uploaded_files only contains completed ones)
            assert len(data["uploaded_files"]) == 5
            
            # Verify different folder structures are represented
            filenames = [f["original_filename"] for f in data["uploaded_files"]]
            assert "root_file.svg" in filenames
            assert "icons/home.svg" in filenames
            assert "graphics/logo.svg" in filenames
    
    def test_zip_folder_organization_workflow_e2e(self, client, test_db_path):
        """Test ZIP folder organization workflow end-to-end."""
        with patch('api.routes.batch.DEFAULT_DB_PATH', test_db_path):
            job_id = "zip_organization_test"
            
            # Create completed batch job
            batch_job = BatchJob(
                job_id=job_id,
                status="completed",
                total_files=3,
                drive_integration_enabled=True
            )
            batch_job.save(test_db_path)
            
            # Create hierarchical Drive folder structure
            drive_metadata = BatchDriveMetadata(
                batch_job_id=job_id,
                drive_folder_id="zip_org_folder_456",
                drive_folder_url="https://drive.google.com/drive/folders/zip_org_folder_456"
            )
            drive_metadata.save(test_db_path)
            
            # Files representing different folder levels
            folder_files = [
                ("level1/file1.svg", "file_level1_1"),
                ("level1/level2/file2.svg", "file_level2_1"), 
                ("level1/level2/level3/file3.svg", "file_level3_1")
            ]
            
            for original_filename, file_id in folder_files:
                file_metadata = BatchFileDriveMetadata(
                    batch_job_id=job_id,
                    original_filename=original_filename,
                    drive_file_id=file_id,
                    drive_file_url=f"https://drive.google.com/file/d/{file_id}/view",
                    upload_status="completed"
                )
                file_metadata.save(test_db_path)
            
            # Test folder organization retrieval
            response = client.get(f"/batch/jobs/{job_id}/drive-info")
            assert response.status_code == 200
            
            data = response.json()
            assert len(data["uploaded_files"]) == 3
            
            # Verify nested structure is maintained in filenames
            nested_files = [f for f in data["uploaded_files"] if "/" in f["original_filename"]]
            assert len(nested_files) == 3
            
            # Verify deepest nesting is preserved
            deep_files = [f for f in data["uploaded_files"] if f["original_filename"].count("/") >= 2]
            assert len(deep_files) >= 1
    
    def test_zip_upload_status_tracking_e2e(self, client, test_db_path):
        """Test ZIP upload status tracking through workflow."""
        with patch('api.routes.batch.DEFAULT_DB_PATH', test_db_path):
            job_id = "zip_status_test"
            
            # Create job with mixed upload statuses
            batch_job = BatchJob(
                job_id=job_id,
                status="uploading",  # Still in progress
                total_files=4,
                drive_integration_enabled=True
            )
            batch_job.save(test_db_path)
            
            drive_metadata = BatchDriveMetadata(
                batch_job_id=job_id,
                drive_folder_id="zip_status_folder_789",
                drive_folder_url="https://drive.google.com/drive/folders/zip_status_folder_789"
            )
            drive_metadata.save(test_db_path)
            
            # Mixed status files
            status_files = [
                ("completed_file.svg", "file_completed", "completed"),
                ("pending_file.svg", "file_pending", "pending"),
                ("uploading_file.svg", "file_uploading", "uploading"),
                ("failed_file.svg", "", "failed")  # No file ID for failed
            ]
            
            for original_filename, file_id, status in status_files:
                file_metadata = BatchFileDriveMetadata(
                    batch_job_id=job_id,
                    original_filename=original_filename,
                    drive_file_id=file_id if file_id else None,
                    drive_file_url=f"https://drive.google.com/file/d/{file_id}/view" if file_id else None,
                    upload_status=status,
                    upload_error="Upload failed due to network error" if status == "failed" else None
                )
                file_metadata.save(test_db_path)
            
            # Test status tracking
            response = client.get(f"/batch/jobs/{job_id}/drive-info")
            assert response.status_code == 200
            
            data = response.json()
            files = data["uploaded_files"]
            
            # Verify different statuses are tracked
            completed = [f for f in files if f["upload_status"] == "completed"]
            pending = [f for f in files if f["upload_status"] == "pending"]
            uploading = [f for f in files if f["upload_status"] == "uploading"]
            failed = [f for f in files if f["upload_status"] == "failed"]
            
            assert len(completed) == 1
            assert len(pending) == 1
            assert len(uploading) == 1
            assert len(failed) == 1
            
            # Verify error details for failed uploads
            failed_file = failed[0]
            assert failed_file["upload_error"] is not None
            assert "network error" in failed_file["upload_error"]
    
    def test_zip_batch_job_status_coordination_e2e(self, client, test_db_path):
        """Test coordination between batch job status and ZIP Drive uploads."""
        with patch('api.routes.batch.DEFAULT_DB_PATH', test_db_path):
            job_id = "zip_coordination_test"
            
            # Create batch job that transitioned through states
            batch_job = BatchJob(
                job_id=job_id,
                status="completed",
                total_files=2,
                drive_integration_enabled=True,
                drive_upload_status="completed"
            )
            batch_job.save(test_db_path)
            
            drive_metadata = BatchDriveMetadata(
                batch_job_id=job_id,
                drive_folder_id="zip_coord_folder_111",
                drive_folder_url="https://drive.google.com/drive/folders/zip_coord_folder_111"
            )
            drive_metadata.save(test_db_path)
            
            # All files successfully processed and uploaded
            coord_files = [
                ("archive/doc1.svg", "file_doc1"),
                ("archive/doc2.svg", "file_doc2")
            ]
            
            for original_filename, file_id in coord_files:
                file_metadata = BatchFileDriveMetadata(
                    batch_job_id=job_id,
                    original_filename=original_filename,
                    drive_file_id=file_id,
                    drive_file_url=f"https://drive.google.com/file/d/{file_id}/view",
                    upload_status="completed"
                )
                file_metadata.save(test_db_path)
            
            # Test overall job status
            job_response = client.get(f"/batch/jobs/{job_id}")
            assert job_response.status_code == 200
            
            job_data = job_response.json()
            assert job_data["status"] == "completed"
            assert job_data["drive_integration_enabled"] is True
            
            # Test Drive info shows coordination
            drive_response = client.get(f"/batch/jobs/{job_id}/drive-info")
            assert drive_response.status_code == 200
            
            drive_data = drive_response.json()
            assert len(drive_data["uploaded_files"]) == 2
            assert all(f["upload_status"] == "completed" for f in drive_data["uploaded_files"])


if __name__ == "__main__":
    pytest.main([__file__, "-v"])