name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run unit tests
      run: |
        python -m pytest tests/unit/ -v \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-fail-under=80 \
          --junit-xml=test-results-unit.xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          test-results-unit.xml
          htmlcov/

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Generate test fixtures
      run: |
        python tests/data/generators/svg_generator.py tests/data/fixtures/basic all
    
    - name: Run integration tests
      run: |
        python -m pytest tests/integration/ -v \
          --junit-xml=test-results-integration.xml \
          --timeout=300
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: test-results-integration.xml

  visual-regression:
    name: Visual Regression Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name != 'schedule'  # Skip on nightly runs
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Setup golden test data
      run: |
        # In a real scenario, this would download or generate golden reference files
        mkdir -p tests/visual/fixtures/golden_standards
        python tests/data/generators/svg_generator.py tests/visual/fixtures/golden_standards all
    
    - name: Run visual regression tests
      run: |
        python -m pytest tests/visual/ -v \
          --junit-xml=test-results-visual.xml \
          -m "not slow"
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: visual-test-results
        path: test-results-visual.xml

  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'schedule' || github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark
    
    - name: Run performance benchmarks
      run: |
        python -m pytest tests/benchmarks/ -v \
          --benchmark-only \
          --benchmark-json=benchmark-results.json
    
    - name: Store benchmark result
      uses: benchmark-action/github-action-benchmark@v1
      if: github.event_name == 'push'
      with:
        tool: 'pytest'
        output-file-path: benchmark-results.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true

  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Run linting
      run: |
        flake8 src tests --max-line-length=120 --extend-ignore=E203,W503
    
    - name: Run type checking
      run: |
        mypy src --ignore-missing-imports
    
    - name: Run security checks
      run: |
        bandit -r src -f json -o bandit-report.json || true
    
    - name: Upload security report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-report
        path: bandit-report.json

  cross-platform-tests:
    name: Cross-Platform Tests
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: [3.11]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run core tests
      run: |
        python -m pytest tests/unit/utils/ tests/unit/converters/ -v \
          --junit-xml=test-results-${{ matrix.os }}.xml
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: cross-platform-results-${{ matrix.os }}
        path: test-results-${{ matrix.os }}.xml

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, visual-regression, code-quality, cross-platform-tests]
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
      with:
        path: test-results
    
    - name: Generate test summary
      run: |
        echo "# Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Visual Regression | ${{ needs.visual-regression.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.code-quality.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Cross-Platform | ${{ needs.cross-platform-tests.result }} |" >> $GITHUB_STEP_SUMMARY
    
    - name: Check overall test status
      run: |
        if [[ "${{ needs.unit-tests.result }}" == "failure" || "${{ needs.integration-tests.result }}" == "failure" || "${{ needs.code-quality.result }}" == "failure" ]]; then
          echo "❌ Critical tests failed"
          exit 1
        else
          echo "✅ All critical tests passed"
        fi